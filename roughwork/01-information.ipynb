{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: rgb(0, 91, 94);\">Information</h1>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "In this notebook we will discuss information and what it might mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Data vs Information</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "\n",
    "- How would you describe the difference between information and data?\n",
    "- Does all data contain information?\n",
    "- Is data analysis about extracting information from data?\n",
    "- Is it about some notion of *insight*?\n",
    "- What about the idea of the signal versus the noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Claude Shannon</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "<img style=\"border: 1px solid #ff791e; max-width: 200px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/99/ClaudeShannon_MFO3807.jpg\" alt=\"Claude Shannon\"></img>\n",
    "\n",
    "- In 1948, Shannon published the seminal work in information theory.\n",
    "- He discussed how information might be viewed.\n",
    "- The work has formed the basis for modern communications.\n",
    "- One of the most interesting aspects of the work is that it largely ignores any notion of underlying meaning in signals.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a style=\"color: #ff791e\" href=\"https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\">\n",
    "    <i>A Mathematical Theory of Communication</i>; Claude Shannon;<br>The Bell System Technical Journal, Vol. 27, pp. 379–423, 623–656, July, October, 1948.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Alice in Wonderland</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "Let's compare two sources of data and consider which might contain more information.\n",
    "\n",
    "We'll start with Alice in Wonderland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make HTTP requests for internet reqources.\n",
    "import urllib.request\n",
    "\n",
    "# The URL of a text version of Alice in Wonderland.\n",
    "book_url = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
    "\n",
    "# Get the book.\n",
    "book = list(urllib.request.urlopen(book_url))\n",
    "\n",
    "# Decode the lines and strip line endings.\n",
    "book = [line.decode('utf-8-sig').strip() for line in book]\n",
    "\n",
    "# Get a sample paragraph - I looked for this by hand.\n",
    "paragraph = ' '.join(book[58:63])\n",
    "\n",
    "# Show the paragraph.\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean it up a bit for fairness compared to the next source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's lower-case it.\n",
    "alice = paragraph.lower()\n",
    "\n",
    "# All letters and a space.\n",
    "chars = 'abcdefghijklmnopqrstuvwxyz '\n",
    "\n",
    "# And strip anything that is not a letter or space.\n",
    "alice = ''.join([c for c in alice if c in chars])\n",
    "\n",
    "# Show the paragraph now.\n",
    "print(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Random Strings</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "Now, let's generate a random sequence of letters compared of the same length.\n",
    "\n",
    "Documentation: https://docs.python.org/3/library/random.html#functions-for-sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For doing (pseudo-)random things in Python.\n",
    "import random\n",
    "\n",
    "# Print a random character, for example.\n",
    "print(random.choice(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of alice.\n",
    "N = len(alice)\n",
    "\n",
    "# Generate N random characters from chars.\n",
    "gener = random.choices(chars, k=N)\n",
    "\n",
    "# Join them together in a string.\n",
    "gener = ''.join(gener)\n",
    "\n",
    "# Print.\n",
    "print(gener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Comparison</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "- Which source do you think contains more information?\n",
    "\n",
    "- In a *meaningful* sense, the Alice in Wonderland paragraph means more to us as humans.\n",
    "\n",
    "- However, Shannon suggests that, as a source of information, the random text has the *capacity* to represent more information.\n",
    "\n",
    "- This is because the Alice in Wonderland example has to follow the rules and conventions of the English language.\n",
    "\n",
    "- If you were observing the sources a character at a time - let's say you receive one every second - you would be more *surprised* by the content of the random source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Building Random Words</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "As an example for thinking about information content, Shannon proposed generating strings of letters based on their propbabilities in the english language.\n",
    "\n",
    "For starters, let's re-build our string but *weight* the characters by their proportions in Alice in Wonderland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the whole book in one big string.\n",
    "sbook = ''.join(book[26:]).lower()\n",
    "\n",
    "# Create the weights - count the occurences of each character in the whole book.\n",
    "weights = [sbook.count(c) for c in chars]\n",
    "\n",
    "# Show the weights.\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a string using those weights.\n",
    "wgenr = random.choices(chars, weights=weights, k=N)\n",
    "\n",
    "# Join them together in a string.\n",
    "wgenr = ''.join(wgenr)\n",
    "\n",
    "# Print.\n",
    "print(wgenr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this including the previous character as part of the weighting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible \n",
    "\n",
    "# Create the weights.\n",
    "twoghts = {c: {d: sbook.count(c + d) for d in chars} for c in chars}\n",
    "\n",
    "# Show the weights.\n",
    "twoghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to start with some letter, so let's just start with the most common letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through our character set.\n",
    "for i in range(len(chars)):\n",
    "    # Print the character and how many times it appears in Alice in Wonderland.\n",
    "    print(f'{chars[i]}: {weights[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with e.\n",
    "twos = 'e'\n",
    "\n",
    "# Do the following 999 times.\n",
    "for i in range(1, 1000):\n",
    "    # Get the weights where the previous character is the last character in twos.\n",
    "    wt = twoghts[twos[-1]]\n",
    "    # Randomly pick the next character using those weights.\n",
    "    nextc = random.choices(chars, weights=weights, k=1)[0]\n",
    "    # Append the character to twos.\n",
    "    twos = twos + nextc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "<i h3 style=\"color: #001a79;\">Remember to do these exercises in your own notebook in your assessment repository.</i>\n",
    "\n",
    "Adapt the code above to generate a 1000 character long string with weights based on the previous two characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Entropy</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "- The idea in the above example suggests the following: the more constraints we put on the randomly generated string, the more it looks like the English language.\n",
    "\n",
    "- Shannon suggested that the information content of a message is related to how surprising it is - which means less constraints.\n",
    "\n",
    "- He defined the information content $I$ of a message $M$ as:\n",
    "\n",
    "$$I(M) = \\log_2 \\frac{1}{p(M)} = - \\log_2 p(M) \\textrm{bits}$$\n",
    "\n",
    "- Here $p(M)$ is the probability of message $M$ appearing.\n",
    "\n",
    "- So, if one bit of information is sent - let's say a 1 with probability of $\\frac{1}{2}$, then the information received is:\n",
    "\n",
    "$$I(M) = - \\log_2 \\frac{1}{2} = \\log_2 2 = 1 \\ \\textrm{bit}$$\n",
    "\n",
    "- What if 1 is received with probability of $\\frac{1}{4}$?\n",
    "\n",
    "$$I(M) = - \\log_2 \\frac{1}{4} = \\log_2 4 = 2 \\ \\textrm{bits}$$\n",
    "\n",
    "- And what if 1 is received with probability of $\\frac{3}{4}$?\n",
    "\n",
    "$$I(M) = - \\log_2 \\frac{3}{4} = \\log_2 \\frac{4}{3} \\approx 0.41 \\ \\textrm{bits}$$\n",
    "\n",
    "- Of course, if a 1 is sent with probability of $\\frac{3}{4}$ then a 0 is sent with a probability $\\frac{1}{4}$, so what is the information content overall?\n",
    "\n",
    "- Shannon suggested using the average information content for all possible messages:\n",
    "\n",
    "$$ H = -\\sum p(x) \\log_2 p(x) \\ \\textrm{bits}$$\n",
    "\n",
    "- For a $50/50$ bit this gives:\n",
    "\n",
    "$$ H = - ( 0.5 \\times \\log_2 \\frac{1}{2} + 0.5 \\times \\log_2 \\frac{1}{2} ) = 1 \\ \\textrm{bit} $$\n",
    "\n",
    "- However, for a $25/75$ bit:\n",
    "\n",
    "$$ H = - ( 0.75 \\times \\log_2 \\frac{3}{4} + 0.5 \\times \\log_2 \\frac{1}{4} ) \\approx 0.8 \\ \\textrm{bits} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "# Numerical calculations.\n",
    "import numpy as np\n",
    "\n",
    "# A range of possible p values.\n",
    "# We have to avoid 0.0 and 1.0 because of the log.\n",
    "p_of_1 = np.linspace(0.01, 0.99, 1000)\n",
    "# Calculate the entropy.\n",
    "entropy = - (p_of_1 * np.log2(p_of_1) + (1.0 - p_of_1) * np.log2(1.0 - p_of_1))\n",
    "\n",
    "# Create the actual plot.\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(p_of_1, entropy, color='#ff791e')\n",
    "\n",
    "# Label the axes.\n",
    "ax.set_xlabel('Probability the bit is 1, not 0.')\n",
    "ax.set_ylabel('Entropy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">John Tukey</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "As a side note, Shannon credits John Tukey with coining the term bit:\n",
    "\n",
    "https://en.wikipedia.org/wiki/John_Tukey\n",
    "\n",
    "<img style=\"border: 1px solid #ff791e; max-width: 200px;\" src=\"https://upload.wikimedia.org/wikipedia/en/e/e9/John_Tukey.jpg\" alt=\"John Tukey\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Passwords</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "<h2 style=\"color: rgb(0, 91, 94);\">End</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
